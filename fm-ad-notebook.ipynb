{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power Up Research Software Development with Github Copilot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will be pulling data from the [Registry of Open Data on AWS](https://registry.opendata.aws/). The Registry of Open Data on AWS (RODA) makes it easy for people to find datasets that are publicly available through AWS.\n",
    "\n",
    "You will also be using various features GitHub Copilot to help you with your data exploration and data analysis processes.\n",
    "\n",
    "\n",
    "For this workshop, you will be analyzing the Foundation Medicine Adult Cancer Clinical Dataset.\n",
    "\n",
    "- [Link to instructions on how to access the dataset via AWS.](https://aws.amazon.com/marketplace/pp/prodview-suzlfg5oc67uy?sr=0-120&ref_=beagle&applicationId=AWSMPContessa)\n",
    "\n",
    "- [Link to the dataset's documentation.](https://gdc.cancer.gov/about-gdc/contributed-genomic-data-cancer-research/foundation-medicine/foundation-medicine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from the [dataset's documentation](https://gdc.cancer.gov/about-gdc/contributed-genomic-data-cancer-research/foundation-medicine):\n",
    "> The Foundation Medicine Adult Cancer Clinical Dataset (FM-AD) is a study conducted by Foundation Medicine Inc (FMI).\n",
    "Genomic profiling data for approximately 18,000 adult patients with a diverse array of cancers was generated using FoundationeOne, FMI's commercially available, comprehensive genomic profiling assay. This dataset contains open Clinical and Biospecimen data.\n",
    "\n",
    "> The dataset is described in the accompanying publication: Hartmaier R.J. et al, “High-Throughput Genomic Profiling of Adult Solid Tumors Reveals Novel Insights into Cancer Pathogenesis”, Cancer Res. 2017 May 1;77(9):2464-2475 http://cancerres.aacrjournals.org/content/77/9/2464.long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You are not expected to read the accompanying publication for this workshop. The notebook will help guide you in understanding the dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Pre-requisites\n",
    "To achieve the desired outcome of your analysis, the conda environment.yml file has installed the following packages:\n",
    "\n",
    "- [NumPy](https://numpy.org/): Fundamental package for numerical computing with support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions.\n",
    "- [pandas](https://pandas.pydata.org/): Provides high-performance, easy-to-use data structures and data analysis tools for working with structured data, such as data frames.\n",
    "- [matplotlib](https://matplotlib.org/): Comprehensive library for creating static, animated, and interactive visualizations in Python. It's often used for creating plots, charts, and graphs.\n",
    "- [Seaborn](https://seaborn.pydata.org/): Built on top of matplotlib, Seaborn provides a high-level interface for creating attractive and informative statistical graphics.\n",
    "- [scikit-learn](https://scikit-learn.org/): Simple and efficient tools for data mining and data analysis. It includes a wide range of machine learning algorithms for classification, regression, clustering, and more.\n",
    "- [awscli](https://aws.amazon.com/cli/): The AWS Command Line Interface (AWS CLI) is a tool for managing AWS services and resources via the command line.\n",
    "- [ipykernel](https://pypi.org/project/ipykernel/): A package that provides the IPython kernel for Jupyter. It allows Jupyter notebooks to execute Python code in an interactive and modular way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check to see if we have these packages installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "def check_packages(packages):\n",
    "    for package in packages:\n",
    "        if importlib.util.find_spec(package) is None:\n",
    "            print(f\"{package} not found.\")\n",
    "        else:\n",
    "            print(f\"{package} is already installed.\")\n",
    "\n",
    "# List of packages to check and install\n",
    "packages = ['numpy', 'pandas', 'matplotlib', 'seaborn', 'sklearn', 'awscli', 'ipykernel']\n",
    "\n",
    "\n",
    "# Call the function\n",
    "check_packages(packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 View S3 bucket content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --no-sign-request s3://gdc-fm-ad-phs001179-2-open/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to explore the folders within the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --no-sign-request s3://gdc-fm-ad-phs001179-2-open/2bec6dfb-5acd-4174-bc50-a00c567d8f33/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Download all the files within the S3 bucket.\n",
    "Make sure that you are in the right directory before running the next command. The command is commented by default to prevent accidental download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show current working directory\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir fm-ad-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to fm-ad-files\n",
    "%cd fm-ad-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show current working directory\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 sync --no-sign-request s3://gdc-fm-ad-phs001179-2-open/ ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 View the file and folder structure of what has been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all of the files and folders in the current directory in hiercical structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using python, show the number of folders and show the number of .tsv files in the current notebook's directory and subdirectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Prepare the dataset for exploration and analysis.\n",
    "Currently, the dataset is distributed into many .tsv files. We want to combine those files into one object. First we will verify that all the .tsv files share unique identifier column(s). Then we can combine the .tsv files into a single data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the common columns amongst all of the .tsv files in the current directory and subdirectories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the .tsv files into a single dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Basic data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the display to show all columns and most rows\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)  # or 1000\n",
    "pd.set_option(\"display.max_rows\", None)  # or 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first few records of the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the dataframe's dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show descriptive statistics of the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the columns and their data types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the number of missing values in each column in desceding order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to missing values, there are columns with 'Unknown' values in string format to represent missing values.\n",
    "\n",
    "Let's take a look at which columns have 'Unknown' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# show which columns have the value 'Unknown' in them and show how many each column has in descending order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the number of unique values in each column in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show 5 unique values of columns with unique values less than 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many records share the same case_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the records with the case_id 40e57344-a8ad-4de4-92e4-6e681c0593b7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we handle duplicate records, let's normalize the notation for missing values first.\n",
    "\n",
    "Currently, missing values are listed as UNKNOWN or NaN. Let's convert them all to NaN for uniforimity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change values 'Unknown' to NaN in the dataframe using numpy and create a new dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any columns still have the value 'Unknown' in them and show how many each column has in descending order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the records that have the value 'Unkown' in the 'diagnoses.site_of_resection_or_biopsy' column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 Handling duplicate records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start checking to see if there are duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the number of duplicate records in the dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate records in the dataframe and create a new dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show dataframe shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next step, refer to the results we gathered from the table with number of unique values in each column.\n",
    "\n",
    "Create a prompt to list the columns with more than 18000 unique values not including the age column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the name of columns that have more than 18000 unique values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the columns seem to server the role of being a unique identifier (UID). You only need one UID. Let's drop the other UIDs except for case_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the name of the columns that have the 'id' in their name except for the column case_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns specified in id_columns from the dataframe and create a new dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the shape of the new dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the columns in the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show columns that have more than 18000 unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can remove the columns above (except for the case_id column) if you think it'll make your analysis easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the top 5 unique values of the columns that have more than 18000 unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2.1, you saw that there were records that shared the same case_id.\n",
    "\n",
    "Let's check if there are any other records share a case_id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many records share the same case_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a bar graph with the x axis as the number of records shared by case_id and the y axis as the number of records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the instance where a case_id is shared between records.\n",
    "\n",
    "Create a prompt below to generate code to show you records that shares a case_id different from the case_id in 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the records with the case_id aff95088-8760-46d2-a404-b545807e0735"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output above, it looks like these records compliment each other. In the column that a record has a value, the other has a null value, and vice versa. This is most likely due to how the data is gathered.\n",
    "\n",
    "Let's try to prove if this is the case for all records that share a case_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt for GitHub Copilot chat:\n",
    "\n",
    "Write Python code that verifies whether the records with the case_id aff95088-8760-46d2-a404-b545807e0735 complement each other in terms of missing values after the first four columns. The code should output whether these records, when combined, fill in each other's missing values like a puzzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python code that verifies whether the records with the case_id aff95088-8760-46d2-a404-b545807e0735 complement one another in terms of missing values after the first four columns. The code should output whether these records, when combined, fill in each other's missing values like a puzzle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same prove for above but for all the records in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'case_id' and take the first non-null value for each group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the shape and the first few records of our dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show number of unique values in each column in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if there are still any empty values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if there are any null values in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the number unique values of the columns that have null values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Normalizing age column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by normalizing the age column. As you will see, the age column does not represent age in years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe stats on diagnoses.age_at_diagnosis column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the publication associated with this dataset, the youngest age of the participant is 19.\n",
    "\n",
    "Let's do some basic math to calulate our normalization factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6947/19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32493/19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe, create a new column 'diagnoses.age_at_diagnosis_years' by dividing 'diagnoses.age_at_diagnosis' by 365, 0and drop the 'diagonses.age_at_diagnosis' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many records that have the value of 'diagnosis.age_at_diagnosis_years' greater or equal to 89\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the record with 'diagnosis.age_at_diagnosis_years' greater or equal to 89\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round down the age column and convert to integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show statistical summary of the age column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bar graph of the age column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's share with GitHub copilot chat the columns in our dataset and what visualizations and correlations we can create from these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list columns of the dataframe and datatype in json format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Additional analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
