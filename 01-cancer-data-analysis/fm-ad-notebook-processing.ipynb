{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power Up Research Software Development with Github Copilot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [fm-ad-notebook-exploration.ipynb](fm-ad-notebook-exploration.ipynb), we conducted various data exploration techniques to gain a deeper understanding of the dataset. Now equipped to make informed decisions regarding data cleaning. Let's load the CSV file generated from the [fm-ad-notebook-exploration.ipynb](fm-ad-notebook-exploration.ipynb) and reference it as a dataframe so that we can start working with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the combined_data.csv to dataframe called combined_df\n",
    "import pandas as pd\n",
    "\n",
    "combined_df = pd.read_csv('combined_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Set output display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To effectively view and analyze the dataset, we need to configure pandas to display all columns and most rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)  # or 1000\n",
    "pd.set_option(\"display.max_rows\", None)  # or 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Dropping columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the dataset documentation, there are 18,004 records in the study. Additionally, results from section 2.6 of [fm-ad-notebook-exploration.ipynb](fm-ad-notebook-exploration.ipynb) indicate that the `case_id` column has 18,004 unique values.\n",
    "\n",
    "Moreover, other columns also have 18,004 unique values. These columns likely serve as unique identifiers similar to the `case_id` column, making them redundant.\n",
    "\n",
    "Let's create a prompt to identify which of these columns fit this criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the name of columns that have more than or equal to 18004 unique values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe and drop columns that have more than or equal to 18004 unique values. however, do not drop the 'case_id' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the shape of the new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the columns in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Normalizing missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we handle duplicate records, let's normalize the notation for missing values first. Currently, missing values are listed as 'Unkown' or NaN. Let's convert them all to NaN for uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change values 'Unknown' to NaN in the dataframe using numpy and create a new dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should verify if the operation above was done successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any columns still have the value 'Unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Handling duplicate records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in section 2.7 of the `fm-ad-notebook-exploration.ipynb` notebook that there were duplicate records. Let's go ahead and drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate records in the dataframe and create a new dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the dimensions of the new dataframe. If the duplicates were dropped succesfully, there should be half the amount of records in the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show dataframe shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 2.7, you saw that there were records that shared the same case_id. Let's check if there are any other records share a case_id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many records share the same case_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a bar graph with the x axis as the number of records shared by case_id and the y axis as the number of records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the instance where a case_id is shared between records.\n",
    "\n",
    "Create a prompt below to generate code to show you records that shares a case_id different from the case_id in section 2.7 of [fm-ad-notebook-exploration.ipynb](fm-ad-notebook-exploration.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the records with the case_id aff95088-8760-46d2-a404-b545807e0735"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Python code that selects the two records with the same 'case_id' value of 'aff95088-8760-46d2-a404-b545807e0735'.\n",
    "# Display these records for visual inspection. Then, verify that these records complement each other in terms of null and non-null values for all columns after the first four columns.\n",
    "# In other words, if one record has NaN values in a column, the other record should have non-NaN values in that same column, and vice versa.\n",
    "# If the two records complement each other, print \"The two records complement each other.\" Otherwise, print \"The two records do not complement each other.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output above, it looks like these records compliment each other. We need to verify that all records with the same `case_id` perfectly complement each other in terms of null and non-null values. If one record has NaN values in a column, the other records should have non-NaN values in that same column, and vice versa. This step ensures data completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a Python code snippet that iterates over all unique 'case_id' values. For each 'case_id', select all records associated with that 'case_id'.\n",
    "# Verify that these records complement each other in terms of null and non-null values for all columns after the first four columns.\n",
    "# In other words, if one record has NaN values in a column, the other records should have non-NaN values in that same column, and vice versa.\n",
    "# Print a dictionary where each 'case_id' is a key and the corresponding value is a boolean indicating whether all records with that 'case_id' perfectly complement each other in terms of null and non-null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine records with the same `case_id` by taking the first non-null value for each group. This step consolidates the data into a more concise format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine records with the same 'case_id' and take the first non-null value for each group. Then create a new dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the shape and the first few records of our dataframe to ensure the data consolidation was successful. A success should show that the dataframe has 18,004 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the shape of the new dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure there are no duplicate records in the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the number of duplicate records in the new dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the number of unique values in each column to identify any potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show number of unique values in each column in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there are still any empty values in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if there are any null values in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the number of unique values in the columns that have null values to understand the extent of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the number unique values of the columns that have null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 Normalizing age column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by normalizing the diagnoses.age_at_diagnosis column which represents age of participants. As you will see, the diagnoses.age_at_diagnosis column does not represent age in years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe stats on diagnoses.age_at_diagnosis column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the publication associated with this dataset, the youngest age of the participant is 19. Let's do some basic math to calulate our normalization factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6947/19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe, create a new column 'diagnoses.age_at_diagnosis_years' by dividing 'diagnoses.age_at_diagnosis' by 365, and drop the 'diagonses.age_at_diagnosis' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many records that have the value of 'diagnosis.age_at_diagnosis_years' greater or equal to 89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the record with 'diagnosis.age_at_diagnosis_years' greater or equal to 89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round down the diagnoses.age_at_diagnosis_years column and convert to integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the range of the new age column is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show statistical summary of the diagnoses.age_at_diagnosis_years column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Save cleaned dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the cleaned dataframe to a new CSV file named combined_data_cleaned.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to a new csv file named combined_data_cleaned.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
